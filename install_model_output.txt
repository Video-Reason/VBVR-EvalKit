
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing ALL models
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: ltx-video
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: ltx-video
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: ltx-video
   âœ… Virtual environment created: ltx-video

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Weights download on first run
   âœ… ltx-video setup complete
   âœ… ltx-video installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: ltx-video-13b-distilled
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: ltx-video-13b-distilled
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: ltx-video-13b-distilled
   âœ… Virtual environment created: ltx-video-13b-distilled

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Weights download on first run
   âœ… ltx-video-13b-distilled setup complete
   âœ… ltx-video-13b-distilled installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: svd
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: svd
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: svd
   âœ… Virtual environment created: svd

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Weights download on first run
   âœ… svd setup complete
   âœ… svd installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: morphic-frames-to-video
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: morphic-frames-to-video
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: morphic-frames-to-video
   âœ… Virtual environment created: morphic-frames-to-video

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â­ï¸  Wan2.2-I2V-A14B weights exist
   â­ï¸  Morphic LoRA weights exist

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Creating Symlinks
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â­ï¸  Weights symlink already exists
   âœ… morphic-frames-to-video setup complete
   âœ… morphic-frames-to-video installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: hunyuan-video-i2v
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: hunyuan-video-i2v
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: hunyuan-video-i2v
   âœ… Virtual environment created: hunyuan-video-i2v

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Weights download on first run
   âœ… hunyuan-video-i2v setup complete
   âœ… hunyuan-video-i2v installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: dynamicrafter-256
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
System Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Checking FFmpeg system dependencies...
   âœ… FFmpeg libraries already installed

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: dynamicrafter-256
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: dynamicrafter-256
   âœ… Virtual environment created: dynamicrafter-256

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â­ï¸  Checkpoint exists: model.ckpt
   âœ… dynamicrafter-256 setup complete
   âœ… dynamicrafter-256 installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: dynamicrafter-512
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
System Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Checking FFmpeg system dependencies...
   âœ… FFmpeg libraries already installed

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: dynamicrafter-512
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: dynamicrafter-512
   âœ… Virtual environment created: dynamicrafter-512

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â­ï¸  Checkpoint exists: model.ckpt
   âœ… dynamicrafter-512 setup complete
   âœ… dynamicrafter-512 installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: dynamicrafter-1024
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
System Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Checking FFmpeg system dependencies...
   âœ… FFmpeg libraries already installed

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: dynamicrafter-1024
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: dynamicrafter-1024
   âœ… Virtual environment created: dynamicrafter-1024

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â­ï¸  Checkpoint exists: model.ckpt
   âœ… dynamicrafter-1024 setup complete
   âœ… dynamicrafter-1024 installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: videocrafter2-512
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
System Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Checking FFmpeg system dependencies...
   âœ… FFmpeg libraries already installed

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: videocrafter2-512
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: videocrafter2-512
   âœ… Virtual environment created: videocrafter2-512

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â­ï¸  Checkpoint exists: model.ckpt
   âœ… videocrafter2-512 setup complete
   âœ… videocrafter2-512 installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: cogvideox-5b-i2v
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
System Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Checking FFmpeg system dependencies...
   âœ… FFmpeg libraries already installed

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: cogvideox-5b-i2v
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: cogvideox-5b-i2v
   âœ… Virtual environment created: cogvideox-5b-i2v

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Model Weights
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Model weights will be downloaded on first run (~11GB)
   ğŸ“Œ HuggingFace repo: THUDM/CogVideoX-5b-I2V
   ğŸ“Œ Cache location: ~/.cache/huggingface/hub/models--THUDM--CogVideoX-5b-I2V
   âœ… cogvideox-5b-i2v setup complete
   ğŸ“Œ Generated videos: 6 seconds (49 frames @ 8fps) at 720x480 resolution
   ğŸ“Œ GPU Memory: ~10GB with optimizations (sequential offload + VAE tiling)
   âœ… cogvideox-5b-i2v installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: cogvideox1.5-5b-i2v
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
System Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Checking FFmpeg system dependencies...
   âœ… FFmpeg libraries already installed

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: cogvideox1.5-5b-i2v
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: cogvideox1.5-5b-i2v
   âœ… Virtual environment created: cogvideox1.5-5b-i2v

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Model Weights
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Model weights will be downloaded on first run (~11GB)
   ğŸ“Œ HuggingFace repo: THUDM/CogVideoX1.5-5B-I2V
   ğŸ“Œ Cache location: ~/.cache/huggingface/hub/models--THUDM--CogVideoX1.5-5B-I2V
   âœ… cogvideox1.5-5b-i2v setup complete
   ğŸ“Œ Generated videos: 10 seconds (81 frames @ 16fps) at 1360x768 resolution
   ğŸ“Œ GPU Memory: ~10GB with optimizations (sequential offload + VAE tiling)
   âœ… cogvideox1.5-5b-i2v installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: sana-video-2b-480p
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: sana-video-2b-480p
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: sana-video-2b-480p
   âœ… Virtual environment created: sana-video-2b-480p

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Weights download on first run via HuggingFace
   âœ… sana-video-2b-480p setup complete
   âœ… sana-video-2b-480p installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: sana-video-2b-longlive
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: sana-video-2b-longlive
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: sana-video-2b-longlive
   âœ… Virtual environment created: sana-video-2b-longlive

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Weights download on first run via HuggingFace
   âœ… sana-video-2b-longlive setup complete
   âœ… sana-video-2b-longlive installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: wan-2.2-i2v-a14b
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: wan
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: wan
   âœ… Virtual environment created: wan

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Weights download on first run
   âœ… wan setup complete
   âœ… wan-2.2-i2v-a14b installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: luma-ray-2
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: luma-ray-2
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: luma-ray-2
   âœ… Virtual environment created: luma-ray-2

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API Configuration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… LUMA_API_KEY configured (luma-aa2...25b3)
   âœ… luma-ray-2 setup complete
   âœ… luma-ray-2 installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: luma-ray-flash-2
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Creating virtual environment: luma-ray-flash-2
   âœ… Virtual environment created: luma-ray-flash-2

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API Configuration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… LUMA_API_KEY configured (luma-aa2...25b3)
   âœ… luma-ray-flash-2 setup complete
   âœ… luma-ray-flash-2 installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: veo-2
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: veo-2
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: veo-2
   âœ… Virtual environment created: veo-2

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API Configuration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… GEMINI_API_KEY configured
   âœ… veo-2 setup complete
   âœ… veo-2 installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: veo-3.0-generate
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: veo-3.0-generate
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: veo-3.0-generate
   âœ… Virtual environment created: veo-3.0-generate

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API Configuration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… GEMINI_API_KEY configured
   âœ… veo-3.0-generate setup complete
   âœ… veo-3.0-generate installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: veo-3.1-fast
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Creating virtual environment: veo-3.1-fast
   âœ… Virtual environment created: veo-3.1-fast

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ERROR: Ignored the following yanked versions: 1.6.0, 1.12.0, 1.16.0
ERROR: Could not find a version that satisfies the requirement google-genai==1.0.1 (from versions: 0.0.1, 0.1.0, 0.2.0, 0.2.1, 0.2.2, 0.3.0, 0.4.0, 0.5.0, 0.6.0, 0.7.0, 0.8.0, 1.0.0rc0, 1.0.0, 1.1.0, 1.2.0, 1.3.0, 1.4.0, 1.5.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.12.1, 1.13.0, 1.14.0, 1.15.0, 1.16.1, 1.17.0, 1.18.0, 1.19.0, 1.20.0, 1.21.0, 1.21.1, 1.22.0, 1.23.0, 1.24.0, 1.25.0, 1.26.0, 1.27.0, 1.28.0, 1.29.0, 1.30.0, 1.31.0, 1.32.0, 1.33.0, 1.34.0, 1.35.0, 1.36.0, 1.37.0, 1.38.0, 1.39.0, 1.39.1, 1.40.0, 1.41.0, 1.42.0, 1.43.0, 1.44.0, 1.45.0, 1.46.0, 1.47.0, 1.48.0, 1.49.0, 1.50.0, 1.50.1, 1.51.0, 1.52.0, 1.53.0, 1.54.0, 1.55.0)
ERROR: No matching distribution found for google-genai==1.0.1
   âŒ veo-3.1-fast installation failed

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: wavespeed-wan-2.1-i2v-480p
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: wavespeed-wan2.1
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: wavespeed-wan2.1
   âœ… Virtual environment created: wavespeed-wan2.1

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Weights download on first run
   âœ… wavespeed-wan2.1 setup complete
   âœ… wavespeed-wan-2.1-i2v-480p installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: runway-gen4-turbo
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: runway-gen4-turbo
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: runway-gen4-turbo
   âœ… Virtual environment created: runway-gen4-turbo

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API Configuration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… RUNWAYML_API_SECRET configured (key_ff2e...37ad)
   âœ… runway-gen4-turbo setup complete
   âœ… runway-gen4-turbo installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: openai-sora-2
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Creating virtual environment: openai-sora-2
   âœ… Virtual environment created: openai-sora-2

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API Configuration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… OPENAI_API_KEY configured (sk-proj-..._1cA)
   âœ… openai-sora-2 setup complete
   âœ… openai-sora-2 installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Validation Phase
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: ltx-video
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating ltx-video... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): ltx-video

ğŸ” Verifying 1 model(s) for testing...
   âœ… ltx-video: LTX-Video
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - ltx-video
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: LTX-Video (ltx-video)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001
      â­ï¸  Skipped (existing output: ltx-video_tests_0001_20251217_002546)
    [2/2] Processing: tests_0002
      â­ï¸  Skipped (existing output: ltx-video_tests_0002_20251217_002627)

  ğŸ“Š Model LTX-Video Summary: 0 completed, 0 failed, 2 skipped in 0h 0m 0s

â±ï¸  Sequential execution completed in 0h 0m 0s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 0 (0.0%)
   Skipped: 2 (100.0%)
   â±ï¸ Duration: 0h 0m 0s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 0 failed | â­ï¸  2 skipped

ğŸ¤– Results by Model:
   ltx-video: âœ… 0 | âŒ 0 | â­ï¸  2

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âœ… ltx-video: 2 videos generated âœ“

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: ltx-video-13b-distilled
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating ltx-video-13b-distilled... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): ltx-video-13b-distilled

ğŸ” Verifying 1 model(s) for testing...
   âœ… ltx-video-13b-distilled: LTX-Video
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - ltx-video-13b-distilled
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: LTX-Video (ltx-video-13b-distilled)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001
      â­ï¸  Skipped (existing output: ltx-video-13b-distilled_tests_0001_20251217_002640)
    [2/2] Processing: tests_0002
      â­ï¸  Skipped (existing output: ltx-video-13b-distilled_tests_0002_20251217_002756)

  ğŸ“Š Model LTX-Video Summary: 0 completed, 0 failed, 2 skipped in 0h 0m 0s

â±ï¸  Sequential execution completed in 0h 0m 0s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 0 (0.0%)
   Skipped: 2 (100.0%)
   â±ï¸ Duration: 0h 0m 0s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 0 failed | â­ï¸  2 skipped

ğŸ¤– Results by Model:
   ltx-video-13b-distilled: âœ… 0 | âŒ 0 | â­ï¸  2

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âœ… ltx-video-13b-distilled: 2 videos generated âœ“

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: svd
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating svd... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): svd

ğŸ” Verifying 1 model(s) for testing...
   âœ… svd: Stable Video Diffusion
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - svd
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: Stable Video Diffusion (svd)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001
      â­ï¸  Skipped (existing output: svd_tests_0001_20251217_002837)
    [2/2] Processing: tests_0002
      â­ï¸  Skipped (existing output: svd_tests_0002_20251217_002929)

  ğŸ“Š Model Stable Video Diffusion Summary: 0 completed, 0 failed, 2 skipped in 0h 0m 0s

â±ï¸  Sequential execution completed in 0h 0m 0s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 0 (0.0%)
   Skipped: 2 (100.0%)
   â±ï¸ Duration: 0h 0m 0s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 0 failed | â­ï¸  2 skipped

ğŸ¤– Results by Model:
   svd: âœ… 0 | âŒ 0 | â­ï¸  2

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âœ… svd: 2 videos generated âœ“

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: morphic-frames-to-video
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating morphic-frames-to-video... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): morphic-frames-to-video

ğŸ” Verifying 1 model(s) for testing...
   âœ… morphic-frames-to-video: Morphic
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - morphic-frames-to-video
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: Morphic (morphic-frames-to-video)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with morphic-frames-to-video
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: morphic-frames-to-video (will be reused for all tasks)

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/morphic-frames-to-video/tests_task/tests_0001/morphic-frames-to-video_tests_0001_20251217_043913
   - Video: data/outputs/pilot_experiment/morphic-frames-to-video/tests_task/tests_0001/morphic-frames-to-video_tests_0001_20251217_043913/video/
   - Question data: data/outputs/pilot_experiment/morphic-frames-to-video/tests_task/tests_0001/morphic-frames-to-video_tests_0001_20251217_043913/question/
   - Metadata: data/outputs/pilot_experiment/morphic-frames-to-video/tests_task/tests_0001/morphic-frames-to-video_tests_0001_20251217_043913/metadata.json
     âŒ Failed: W1217 04:39:20.603000 60405 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] 
W1217 04:39:20.603000 60405 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] *****************************************
W1217 04:39:20.603000 60405 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1217 04:39:20.603000 60405 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] *****************************************
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
        generate(args)generate(args)

  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    generate(args)
    generate(args)  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate

  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
            torch.cuda.set_device(local_rank)    
torch.cuda.set_device(local_rank)torch.cuda.set_device(local_rank)torch.cuda.set_device(local_rank)
    
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device

  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch._C._cuda_setDevice(device)
    RuntimeErrortorch._C._cuda_setDevice(device):     
CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
torch._C._cuda_setDevice(device)

RuntimeError    : torch._C._cuda_setDevice(device)CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
RuntimeError

: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
    
torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

W1217 04:39:36.254000 60405 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 60465 closing signal SIGTERM
W1217 04:39:36.256000 60405 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 60466 closing signal SIGTERM
E1217 04:39:36.420000 60405 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 60467) of binary: /home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/bin/python3
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-12-17_04:39:36
  host      : 192-222-52-138
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 60468)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-12-17_04:39:36
  host      : 192-222-52-138
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 60469)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-12-17_04:39:36
  host      : 192-222-52-138
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 60470)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2025-12-17_04:39:36
  host      : 192-222-52-138
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 60471)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2025-12-17_04:39:36
  host      : 192-222-52-138
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 60472)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-17_04:39:36
  host      : 192-222-52-138
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 60467)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

      âŒ Failed: W1217 04:39:20.603000 60405 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] 
W1217 04:39:20.603000 60405 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] *****************************************
W1217 04:39:20.603000 60405 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1217 04:39:20.603000 60405 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] *****************************************
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
        generate(args)generate(args)

  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    generate(args)
    generate(args)  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate

  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
            torch.cuda.set_device(local_rank)    
torch.cuda.set_device(local_rank)torch.cuda.set_device(local_rank)torch.cuda.set_device(local_rank)
    
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device

  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch._C._cuda_setDevice(device)
    RuntimeErrortorch._C._cuda_setDevice(device):     
CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
torch._C._cuda_setDevice(device)

RuntimeError    : torch._C._cuda_setDevice(device)CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
RuntimeError

: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
    
torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

W1217 04:39:36.254000 60405 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 60465 closing signal SIGTERM
W1217 04:39:36.256000 60405 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 60466 closing signal SIGTERM
E1217 04:39:36.420000 60405 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 60467) of binary: /home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/bin/python3
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-12-17_04:39:36
  host      : 192-222-52-138
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 60468)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-12-17_04:39:36
  host      : 192-222-52-138
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 60469)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-12-17_04:39:36
  host      : 192-222-52-138
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 60470)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2025-12-17_04:39:36
  host      : 192-222-52-138
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 60471)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2025-12-17_04:39:36
  host      : 192-222-52-138
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 60472)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-17_04:39:36
  host      : 192-222-52-138
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 60467)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

    [2/2] Processing: tests_0002

  ğŸ¬ Generating: tests_0002 with morphic-frames-to-video
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0002/first_frame.png
     Prompt: The image shows a horizontally flipped clock. After 2 hours passes on the origin...

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/morphic-frames-to-video/tests_task/tests_0002/morphic-frames-to-video_tests_0002_20251217_043937
   - Video: data/outputs/pilot_experiment/morphic-frames-to-video/tests_task/tests_0002/morphic-frames-to-video_tests_0002_20251217_043937/video/
   - Question data: data/outputs/pilot_experiment/morphic-frames-to-video/tests_task/tests_0002/morphic-frames-to-video_tests_0002_20251217_043937/question/
   - Metadata: data/outputs/pilot_experiment/morphic-frames-to-video/tests_task/tests_0002/morphic-frames-to-video_tests_0002_20251217_043937/metadata.json
     âŒ Failed: W1217 04:39:39.182000 60505 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] 
W1217 04:39:39.182000 60505 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] *****************************************
W1217 04:39:39.182000 60505 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1217 04:39:39.182000 60505 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] *****************************************
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
        generate(args)generate(args)

  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
        generate(args)generate(args)

  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
        torch.cuda.set_device(local_rank)torch.cuda.set_device(local_rank)

      File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)torch.cuda.set_device(local_rank)

  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
        torch._C._cuda_setDevice(device)torch._C._cuda_setDevice(device)

    RuntimeErrortorch._C._cuda_setDevice(device)RuntimeError: 
: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

        torch._C._cuda_setDevice(device)torch._C._cuda_setDevice(device)

RuntimeErrorRuntimeError: : CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


W1217 04:39:50.242000 60505 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 60573 closing signal SIGTERM
W1217 04:39:50.243000 60505 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 60574 closing signal SIGTERM
E1217 04:39:50.457000 60505 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 60575) of binary: /home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/bin/python3
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-12-17_04:39:50
  host      : 192-222-52-138
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 60576)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-12-17_04:39:50
  host      : 192-222-52-138
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 60577)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-12-17_04:39:50
  host      : 192-222-52-138
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 60578)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2025-12-17_04:39:50
  host      : 192-222-52-138
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 60579)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2025-12-17_04:39:50
  host      : 192-222-52-138
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 60580)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-17_04:39:50
  host      : 192-222-52-138
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 60575)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

      âŒ Failed: W1217 04:39:39.182000 60505 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] 
W1217 04:39:39.182000 60505 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] *****************************************
W1217 04:39:39.182000 60505 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1217 04:39:39.182000 60505 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] *****************************************
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
        generate(args)generate(args)

  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
        generate(args)generate(args)

  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
        torch.cuda.set_device(local_rank)torch.cuda.set_device(local_rank)

      File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)torch.cuda.set_device(local_rank)

  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
        torch._C._cuda_setDevice(device)torch._C._cuda_setDevice(device)

    RuntimeErrortorch._C._cuda_setDevice(device)RuntimeError: 
: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

        torch._C._cuda_setDevice(device)torch._C._cuda_setDevice(device)

RuntimeErrorRuntimeError: : CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


W1217 04:39:50.242000 60505 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 60573 closing signal SIGTERM
W1217 04:39:50.243000 60505 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 60574 closing signal SIGTERM
E1217 04:39:50.457000 60505 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 60575) of binary: /home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/bin/python3
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-12-17_04:39:50
  host      : 192-222-52-138
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 60576)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-12-17_04:39:50
  host      : 192-222-52-138
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 60577)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-12-17_04:39:50
  host      : 192-222-52-138
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 60578)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2025-12-17_04:39:50
  host      : 192-222-52-138
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 60579)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2025-12-17_04:39:50
  host      : 192-222-52-138
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 60580)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-17_04:39:50
  host      : 192-222-52-138
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 60575)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================


  ğŸ“Š Model Morphic Summary: 0 completed, 2 failed, 0 skipped in 0h 0m 37s

â±ï¸  Sequential execution completed in 0h 0m 37s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 2 (100.0%)
   Skipped: 0 (0.0%)
   â±ï¸ Duration: 0h 0m 37s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 2 failed | â­ï¸  0 skipped

ğŸ¤– Results by Model:
   morphic-frames-to-video: âœ… 0 | âŒ 2 | â­ï¸  0

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âŒ morphic-frames-to-video: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: hunyuan-video-i2v
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating hunyuan-video-i2v... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): hunyuan-video-i2v

ğŸ” Verifying 1 model(s) for testing...
   âœ… hunyuan-video-i2v: HunyuanVideo
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - hunyuan-video-i2v
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: HunyuanVideo (hunyuan-video-i2v)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with hunyuan-video-i2v
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: hunyuan-video-i2v (will be reused for all tasks)
Warning: HunyuanVideo requires 60-80GB GPU memory for 720p generation

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/hunyuan-video-i2v/tests_task/tests_0001/hunyuan-video-i2v_tests_0001_20251217_043951
   - Video: data/outputs/pilot_experiment/hunyuan-video-i2v/tests_task/tests_0001/hunyuan-video-i2v_tests_0001_20251217_043951/video/
   - Question data: data/outputs/pilot_experiment/hunyuan-video-i2v/tests_task/tests_0001/hunyuan-video-i2v_tests_0001_20251217_043951/question/
   - Metadata: data/outputs/pilot_experiment/hunyuan-video-i2v/tests_task/tests_0001/hunyuan-video-i2v_tests_0001_20251217_043951/metadata.json
     âŒ Failed: df: /home/ubuntu/.triton/autotune: No such file or directory
/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/torch/utils/cpp_extension.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging  # type: ignore[attr-defined]
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/sample_image2video.py", line 8, in <module>
    from hyvideo.config import parse_args
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/config.py", line 4, in <module>
    from .modules.models import HUNYUAN_VIDEO_CONFIG
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/__init__.py", line 1, in <module>
    from .models import HYVideoDiffusionTransformer, HUNYUAN_VIDEO_CONFIG
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/models.py", line 15, in <module>
    from .embed_layers import TimestepEmbedder, PatchEmbed, TextProjection
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/embed_layers.py", line 6, in <module>
    from ..utils.helpers import to_2tuple
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/utils/helpers.py", line 13, in <module>
    from torch.utils.tensorboard import SummaryWriter
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'

      âŒ Failed: df: /home/ubuntu/.triton/autotune: No such file or directory
/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/torch/utils/cpp_extension.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging  # type: ignore[attr-defined]
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/sample_image2video.py", line 8, in <module>
    from hyvideo.config import parse_args
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/config.py", line 4, in <module>
    from .modules.models import HUNYUAN_VIDEO_CONFIG
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/__init__.py", line 1, in <module>
    from .models import HYVideoDiffusionTransformer, HUNYUAN_VIDEO_CONFIG
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/models.py", line 15, in <module>
    from .embed_layers import TimestepEmbedder, PatchEmbed, TextProjection
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/embed_layers.py", line 6, in <module>
    from ..utils.helpers import to_2tuple
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/utils/helpers.py", line 13, in <module>
    from torch.utils.tensorboard import SummaryWriter
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'

    [2/2] Processing: tests_0002

  ğŸ¬ Generating: tests_0002 with hunyuan-video-i2v
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0002/first_frame.png
     Prompt: The image shows a horizontally flipped clock. After 2 hours passes on the origin...
Warning: HunyuanVideo requires 60-80GB GPU memory for 720p generation

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/hunyuan-video-i2v/tests_task/tests_0002/hunyuan-video-i2v_tests_0002_20251217_044009
   - Video: data/outputs/pilot_experiment/hunyuan-video-i2v/tests_task/tests_0002/hunyuan-video-i2v_tests_0002_20251217_044009/video/
   - Question data: data/outputs/pilot_experiment/hunyuan-video-i2v/tests_task/tests_0002/hunyuan-video-i2v_tests_0002_20251217_044009/question/
   - Metadata: data/outputs/pilot_experiment/hunyuan-video-i2v/tests_task/tests_0002/hunyuan-video-i2v_tests_0002_20251217_044009/metadata.json
     âŒ Failed: /home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/torch/utils/cpp_extension.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging  # type: ignore[attr-defined]
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/sample_image2video.py", line 8, in <module>
    from hyvideo.config import parse_args
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/config.py", line 4, in <module>
    from .modules.models import HUNYUAN_VIDEO_CONFIG
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/__init__.py", line 1, in <module>
    from .models import HYVideoDiffusionTransformer, HUNYUAN_VIDEO_CONFIG
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/models.py", line 15, in <module>
    from .embed_layers import TimestepEmbedder, PatchEmbed, TextProjection
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/embed_layers.py", line 6, in <module>
    from ..utils.helpers import to_2tuple
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/utils/helpers.py", line 13, in <module>
    from torch.utils.tensorboard import SummaryWriter
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'

      âŒ Failed: /home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/torch/utils/cpp_extension.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging  # type: ignore[attr-defined]
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/sample_image2video.py", line 8, in <module>
    from hyvideo.config import parse_args
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/config.py", line 4, in <module>
    from .modules.models import HUNYUAN_VIDEO_CONFIG
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/__init__.py", line 1, in <module>
    from .models import HYVideoDiffusionTransformer, HUNYUAN_VIDEO_CONFIG
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/models.py", line 15, in <module>
    from .embed_layers import TimestepEmbedder, PatchEmbed, TextProjection
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/embed_layers.py", line 6, in <module>
    from ..utils.helpers import to_2tuple
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/utils/helpers.py", line 13, in <module>
    from torch.utils.tensorboard import SummaryWriter
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'


  ğŸ“Š Model HunyuanVideo Summary: 0 completed, 2 failed, 0 skipped in 0h 0m 27s

â±ï¸  Sequential execution completed in 0h 0m 27s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 2 (100.0%)
   Skipped: 0 (0.0%)
   â±ï¸ Duration: 0h 0m 27s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 2 failed | â­ï¸  0 skipped

ğŸ¤– Results by Model:
   hunyuan-video-i2v: âœ… 0 | âŒ 2 | â­ï¸  0

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âŒ hunyuan-video-i2v: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: dynamicrafter-256
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating dynamicrafter-256... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): dynamicrafter-256

ğŸ” Verifying 1 model(s) for testing...
   âœ… dynamicrafter-256: DynamiCrafter
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - dynamicrafter-256
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: DynamiCrafter (dynamicrafter-256)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with dynamicrafter-256
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: dynamicrafter-256 (will be reused for all tasks)
DEBUG - Return code: 1
DEBUG - stdout: 
DEBUG - stderr: Traceback (most recent call last):
  File "/tmp/tmpq5kdiwng.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'


âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/dynamicrafter-256/tests_task/tests_0001/dynamicrafter-256_tests_0001_20251217_044018
   - Video: data/outputs/pilot_experiment/dynamicrafter-256/tests_task/tests_0001/dynamicrafter-256_tests_0001_20251217_044018/video/
   - Question data: data/outputs/pilot_experiment/dynamicrafter-256/tests_task/tests_0001/dynamicrafter-256_tests_0001_20251217_044018/question/
   - Metadata: data/outputs/pilot_experiment/dynamicrafter-256/tests_task/tests_0001/dynamicrafter-256_tests_0001_20251217_044018/metadata.json
     âŒ Failed: Traceback (most recent call last):
  File "/tmp/tmpq5kdiwng.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'

      âŒ Failed: Traceback (most recent call last):
  File "/tmp/tmpq5kdiwng.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'

    [2/2] Processing: tests_0002

  ğŸ¬ Generating: tests_0002 with dynamicrafter-256
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0002/first_frame.png
     Prompt: The image shows a horizontally flipped clock. After 2 hours passes on the origin...
DEBUG - Return code: 1
DEBUG - stdout: 
DEBUG - stderr: Traceback (most recent call last):
  File "/tmp/tmpgzgim2ic.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'


âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/dynamicrafter-256/tests_task/tests_0002/dynamicrafter-256_tests_0002_20251217_044026
   - Video: data/outputs/pilot_experiment/dynamicrafter-256/tests_task/tests_0002/dynamicrafter-256_tests_0002_20251217_044026/video/
   - Question data: data/outputs/pilot_experiment/dynamicrafter-256/tests_task/tests_0002/dynamicrafter-256_tests_0002_20251217_044026/question/
   - Metadata: data/outputs/pilot_experiment/dynamicrafter-256/tests_task/tests_0002/dynamicrafter-256_tests_0002_20251217_044026/metadata.json
     âŒ Failed: Traceback (most recent call last):
  File "/tmp/tmpgzgim2ic.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'

      âŒ Failed: Traceback (most recent call last):
  File "/tmp/tmpgzgim2ic.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'


  ğŸ“Š Model DynamiCrafter Summary: 0 completed, 2 failed, 0 skipped in 0h 0m 10s

â±ï¸  Sequential execution completed in 0h 0m 10s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 2 (100.0%)
   Skipped: 0 (0.0%)
   â±ï¸ Duration: 0h 0m 10s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 2 failed | â­ï¸  0 skipped

ğŸ¤– Results by Model:
   dynamicrafter-256: âœ… 0 | âŒ 2 | â­ï¸  0

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âŒ dynamicrafter-256: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: dynamicrafter-512
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating dynamicrafter-512... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): dynamicrafter-512

ğŸ” Verifying 1 model(s) for testing...
   âœ… dynamicrafter-512: DynamiCrafter
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - dynamicrafter-512
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: DynamiCrafter (dynamicrafter-512)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with dynamicrafter-512
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: dynamicrafter-512 (will be reused for all tasks)
DEBUG - Return code: 1
DEBUG - stdout: 
DEBUG - stderr: Traceback (most recent call last):
  File "/tmp/tmp31zvtq60.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'


âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/dynamicrafter-512/tests_task/tests_0001/dynamicrafter-512_tests_0001_20251217_044029
   - Video: data/outputs/pilot_experiment/dynamicrafter-512/tests_task/tests_0001/dynamicrafter-512_tests_0001_20251217_044029/video/
   - Question data: data/outputs/pilot_experiment/dynamicrafter-512/tests_task/tests_0001/dynamicrafter-512_tests_0001_20251217_044029/question/
   - Metadata: data/outputs/pilot_experiment/dynamicrafter-512/tests_task/tests_0001/dynamicrafter-512_tests_0001_20251217_044029/metadata.json
     âŒ Failed: Traceback (most recent call last):
  File "/tmp/tmp31zvtq60.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'

      âŒ Failed: Traceback (most recent call last):
  File "/tmp/tmp31zvtq60.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'

    [2/2] Processing: tests_0002

  ğŸ¬ Generating: tests_0002 with dynamicrafter-512
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0002/first_frame.png
     Prompt: The image shows a horizontally flipped clock. After 2 hours passes on the origin...
DEBUG - Return code: 1
DEBUG - stdout: 
DEBUG - stderr: Traceback (most recent call last):
  File "/tmp/tmpowc_efeb.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'


âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/dynamicrafter-512/tests_task/tests_0002/dynamicrafter-512_tests_0002_20251217_044036
   - Video: data/outputs/pilot_experiment/dynamicrafter-512/tests_task/tests_0002/dynamicrafter-512_tests_0002_20251217_044036/video/
   - Question data: data/outputs/pilot_experiment/dynamicrafter-512/tests_task/tests_0002/dynamicrafter-512_tests_0002_20251217_044036/question/
   - Metadata: data/outputs/pilot_experiment/dynamicrafter-512/tests_task/tests_0002/dynamicrafter-512_tests_0002_20251217_044036/metadata.json
     âŒ Failed: Traceback (most recent call last):
  File "/tmp/tmpowc_efeb.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'

      âŒ Failed: Traceback (most recent call last):
  File "/tmp/tmpowc_efeb.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'


  ğŸ“Š Model DynamiCrafter Summary: 0 completed, 2 failed, 0 skipped in 0h 0m 9s

â±ï¸  Sequential execution completed in 0h 0m 9s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 2 (100.0%)
   Skipped: 0 (0.0%)
   â±ï¸ Duration: 0h 0m 9s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 2 failed | â­ï¸  0 skipped

ğŸ¤– Results by Model:
   dynamicrafter-512: âœ… 0 | âŒ 2 | â­ï¸  0

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âŒ dynamicrafter-512: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: dynamicrafter-1024
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating dynamicrafter-1024... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): dynamicrafter-1024

ğŸ” Verifying 1 model(s) for testing...
   âœ… dynamicrafter-1024: DynamiCrafter
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - dynamicrafter-1024
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: DynamiCrafter (dynamicrafter-1024)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with dynamicrafter-1024
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: dynamicrafter-1024 (will be reused for all tasks)
DEBUG - Return code: 1
DEBUG - stdout: 
DEBUG - stderr: Traceback (most recent call last):
  File "/tmp/tmpo8by1rd9.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'


âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/dynamicrafter-1024/tests_task/tests_0001/dynamicrafter-1024_tests_0001_20251217_044039
   - Video: data/outputs/pilot_experiment/dynamicrafter-1024/tests_task/tests_0001/dynamicrafter-1024_tests_0001_20251217_044039/video/
   - Question data: data/outputs/pilot_experiment/dynamicrafter-1024/tests_task/tests_0001/dynamicrafter-1024_tests_0001_20251217_044039/question/
   - Metadata: data/outputs/pilot_experiment/dynamicrafter-1024/tests_task/tests_0001/dynamicrafter-1024_tests_0001_20251217_044039/metadata.json
     âŒ Failed: Traceback (most recent call last):
  File "/tmp/tmpo8by1rd9.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'

      âŒ Failed: Traceback (most recent call last):
  File "/tmp/tmpo8by1rd9.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'

    [2/2] Processing: tests_0002

  ğŸ¬ Generating: tests_0002 with dynamicrafter-1024
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0002/first_frame.png
     Prompt: The image shows a horizontally flipped clock. After 2 hours passes on the origin...
DEBUG - Return code: 1
DEBUG - stdout: 
DEBUG - stderr: Traceback (most recent call last):
  File "/tmp/tmpp4eg7iu7.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'


âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/dynamicrafter-1024/tests_task/tests_0002/dynamicrafter-1024_tests_0002_20251217_044046
   - Video: data/outputs/pilot_experiment/dynamicrafter-1024/tests_task/tests_0002/dynamicrafter-1024_tests_0002_20251217_044046/video/
   - Question data: data/outputs/pilot_experiment/dynamicrafter-1024/tests_task/tests_0002/dynamicrafter-1024_tests_0002_20251217_044046/question/
   - Metadata: data/outputs/pilot_experiment/dynamicrafter-1024/tests_task/tests_0002/dynamicrafter-1024_tests_0002_20251217_044046/metadata.json
     âŒ Failed: Traceback (most recent call last):
  File "/tmp/tmpp4eg7iu7.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'

      âŒ Failed: Traceback (most recent call last):
  File "/tmp/tmpp4eg7iu7.py", line 14, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/DynamiCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'


  ğŸ“Š Model DynamiCrafter Summary: 0 completed, 2 failed, 0 skipped in 0h 0m 9s

â±ï¸  Sequential execution completed in 0h 0m 9s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 2 (100.0%)
   Skipped: 0 (0.0%)
   â±ï¸ Duration: 0h 0m 9s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 2 failed | â­ï¸  0 skipped

ğŸ¤– Results by Model:
   dynamicrafter-1024: âœ… 0 | âŒ 2 | â­ï¸  0

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âŒ dynamicrafter-1024: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: videocrafter2-512
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating videocrafter2-512... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): videocrafter2-512

ğŸ” Verifying 1 model(s) for testing...
   âœ… videocrafter2-512: VideoCrafter
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - videocrafter2-512
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: VideoCrafter (videocrafter2-512)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with videocrafter2-512
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: videocrafter2-512 (will be reused for all tasks)

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/videocrafter2-512/tests_task/tests_0001/videocrafter2-512_tests_0001_20251217_044048
   - Video: data/outputs/pilot_experiment/videocrafter2-512/tests_task/tests_0001/videocrafter2-512_tests_0001_20251217_044048/video/
   - Question data: data/outputs/pilot_experiment/videocrafter2-512/tests_task/tests_0001/videocrafter2-512_tests_0001_20251217_044048/question/
   - Metadata: data/outputs/pilot_experiment/videocrafter2-512/tests_task/tests_0001/videocrafter2-512_tests_0001_20251217_044048/metadata.json
     âŒ Failed: VideoCrafter inference requires manual setup of model checkpoints. Please refer to the VideoCrafter repository for setup instructions. Original error: Traceback (most recent call last):
  File "/tmp/tmpjvvfbrti.py", line 13, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/VideoCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/VideoCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'

      âŒ Failed: VideoCrafter inference requires manual setup of model checkpoints. Please refer to the VideoCrafter repository for setup instructions. Original error: Traceback (most recent call last):
  File "/tmp/tmpjvvfbrti.py", line 13, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/VideoCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/VideoCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'

    [2/2] Processing: tests_0002

  ğŸ¬ Generating: tests_0002 with videocrafter2-512
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0002/first_frame.png
     Prompt: The image shows a horizontally flipped clock. After 2 hours passes on the origin...

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/videocrafter2-512/tests_task/tests_0002/videocrafter2-512_tests_0002_20251217_044056
   - Video: data/outputs/pilot_experiment/videocrafter2-512/tests_task/tests_0002/videocrafter2-512_tests_0002_20251217_044056/video/
   - Question data: data/outputs/pilot_experiment/videocrafter2-512/tests_task/tests_0002/videocrafter2-512_tests_0002_20251217_044056/question/
   - Metadata: data/outputs/pilot_experiment/videocrafter2-512/tests_task/tests_0002/videocrafter2-512_tests_0002_20251217_044056/metadata.json
     âŒ Failed: VideoCrafter inference requires manual setup of model checkpoints. Please refer to the VideoCrafter repository for setup instructions. Original error: Traceback (most recent call last):
  File "/tmp/tmp6ov0am3f.py", line 13, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/VideoCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/VideoCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'

      âŒ Failed: VideoCrafter inference requires manual setup of model checkpoints. Please refer to the VideoCrafter repository for setup instructions. Original error: Traceback (most recent call last):
  File "/tmp/tmp6ov0am3f.py", line 13, in <module>
    from lvdm.models.samplers.ddim import DDIMSampler
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/VideoCrafter/lvdm/models/samplers/ddim.py", line 5, in <module>
    from lvdm.common import noise_like
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/VideoCrafter/lvdm/common.py", line 80, in <module>
    ckpt = torch.utils.checkpoint.checkpoint
AttributeError: module 'torch.utils' has no attribute 'checkpoint'


  ğŸ“Š Model VideoCrafter Summary: 0 completed, 2 failed, 0 skipped in 0h 0m 10s

â±ï¸  Sequential execution completed in 0h 0m 10s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 2 (100.0%)
   Skipped: 0 (0.0%)
   â±ï¸ Duration: 0h 0m 10s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 2 failed | â­ï¸  0 skipped

ğŸ¤– Results by Model:
   videocrafter2-512: âœ… 0 | âŒ 2 | â­ï¸  0

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âŒ videocrafter2-512: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: cogvideox-5b-i2v
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating cogvideox-5b-i2v... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): cogvideox-5b-i2v

ğŸ” Verifying 1 model(s) for testing...
   âœ… cogvideox-5b-i2v: CogVideoX
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - cogvideox-5b-i2v
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: CogVideoX (cogvideox-5b-i2v)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with cogvideox-5b-i2v
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: cogvideox-5b-i2v (will be reused for all tasks)
Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:02,  1.40it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.40it/s][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.37it/s]
Loading pipeline components...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:01,  2.50it/s]The config attributes {'invert_scale_latents': False} were passed to AutoencoderKLCogVideoX, but are not expected and will be ignored. Please verify your config.json configuration file.
Loading pipeline components...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:01<00:00,  3.73it/s]Loading pipeline components...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:01<00:00,  4.44it/s]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  4.04it/s]
  0%|          | 0/50 [00:00<?, ?it/s]  2%|â–         | 1/50 [00:04<03:19,  4.06s/it]  4%|â–         | 2/50 [00:07<02:59,  3.74s/it]  6%|â–Œ         | 3/50 [00:11<02:50,  3.64s/it]  8%|â–Š         | 4/50 [00:14<02:45,  3.59s/it] 10%|â–ˆ         | 5/50 [00:18<02:41,  3.58s/it] 12%|â–ˆâ–        | 6/50 [00:21<02:36,  3.56s/it] 14%|â–ˆâ–        | 7/50 [00:25<02:33,  3.56s/it] 16%|â–ˆâ–Œ        | 8/50 [00:28<02:30,  3.59s/it] 18%|â–ˆâ–Š        | 9/50 [00:32<02:25,  3.55s/it] 20%|â–ˆâ–ˆ        | 10/50 [00:35<02:20,  3.52s/it] 22%|â–ˆâ–ˆâ–       | 11/50 [00:39<02:19,  3.57s/it] 24%|â–ˆâ–ˆâ–       | 12/50 [00:42<02:14,  3.54s/it] 26%|â–ˆâ–ˆâ–Œ       | 13/50 [00:46<02:10,  3.52s/it] 28%|â–ˆâ–ˆâ–Š       | 14/50 [00:49<02:06,  3.50s/it] 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [00:53<02:04,  3.55s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:57<01:59,  3.52s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [01:00<01:55,  3.51s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [01:03<01:51,  3.50s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [01:07<01:48,  3.49s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [01:11<01:46,  3.54s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [01:14<01:42,  3.52s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [01:18<01:38,  3.51s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [01:21<01:34,  3.49s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [01:25<01:32,  3.54s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [01:28<01:27,  3.52s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [01:32<01:24,  3.51s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [01:35<01:20,  3.50s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [01:39<01:16,  3.49s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [01:42<01:14,  3.54s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [01:46<01:10,  3.52s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [01:49<01:06,  3.51s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [01:53<01:02,  3.49s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [01:56<01:00,  3.54s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [02:00<00:56,  3.52s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [02:03<00:52,  3.51s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [02:07<00:48,  3.50s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [02:10<00:45,  3.49s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [02:14<00:42,  3.54s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [02:17<00:38,  3.52s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [02:21<00:35,  3.51s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [02:24<00:31,  3.49s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [02:28<00:28,  3.54s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [02:31<00:24,  3.52s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [02:35<00:21,  3.51s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [02:38<00:17,  3.50s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [02:42<00:13,  3.49s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [02:45<00:10,  3.54s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [02:49<00:07,  3.52s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [02:52<00:03,  3.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:56<00:00,  3.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:56<00:00,  3.53s/it]
It is recommended to use `export_to_video` with `imageio` and `imageio-ffmpeg` as a backend. 
These libraries are not present in your environment. Attempting to use legacy OpenCV backend to export video. 
Support for the OpenCV backend will be deprecated in a future Diffusers version
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 202, in run
    result = wrapper.generate(image_path, text_prompt, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/cogvideox_inference.py", line 347, in generate
    result = self.service.generate_video(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/cogvideox_inference.py", line 216, in generate_video
    export_to_video(frames, str(output_path), fps=self.config.fps)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/cogvideox-5b-i2v/lib/python3.10/site-packages/diffusers/utils/export_utils.py", line 154, in export_to_video
    return _legacy_export_to_video(video_frames, output_video_path, fps)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/cogvideox-5b-i2v/lib/python3.10/site-packages/diffusers/utils/export_utils.py", line 121, in _legacy_export_to_video
    raise ImportError(BACKENDS_MAPPING["opencv"][1].format("export_to_video"))
ImportError: 
export_to_video requires the OpenCV library but it was not found in your environment. You can install it with pip: `pip
install opencv-python`

   âŒ cogvideox-5b-i2v: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: cogvideox1.5-5b-i2v
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating cogvideox1.5-5b-i2v... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): cogvideox1.5-5b-i2v

ğŸ” Verifying 1 model(s) for testing...
   âœ… cogvideox1.5-5b-i2v: CogVideoX
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - cogvideox1.5-5b-i2v
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: CogVideoX (cogvideox1.5-5b-i2v)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with cogvideox1.5-5b-i2v
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: cogvideox1.5-5b-i2v (will be reused for all tasks)
Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  3.63it/s][A
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  3.85it/s][A
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  3.94it/s][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.14it/s][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.02it/s]
Loading pipeline components...:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  1.15s/it]Loading pipeline components...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:01,  1.88it/s]Loading pipeline components...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:01<00:00,  2.78it/s]Loading pipeline components...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:01<00:00,  2.95it/s]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.76it/s]
  0%|          | 0/50 [00:00<?, ?it/s]  2%|â–         | 1/50 [00:06<05:18,  6.50s/it]  4%|â–         | 2/50 [00:12<04:56,  6.17s/it]  6%|â–Œ         | 3/50 [00:18<04:44,  6.06s/it]  8%|â–Š         | 4/50 [00:24<04:36,  6.01s/it] 10%|â–ˆ         | 5/50 [00:30<04:28,  5.98s/it] 12%|â–ˆâ–        | 6/50 [00:36<04:22,  5.96s/it] 14%|â–ˆâ–        | 7/50 [00:42<04:16,  5.95s/it] 16%|â–ˆâ–Œ        | 8/50 [00:48<04:09,  5.95s/it] 18%|â–ˆâ–Š        | 9/50 [00:53<04:03,  5.94s/it] 20%|â–ˆâ–ˆ        | 10/50 [00:59<03:57,  5.94s/it] 22%|â–ˆâ–ˆâ–       | 11/50 [01:05<03:51,  5.94s/it] 24%|â–ˆâ–ˆâ–       | 12/50 [01:11<03:45,  5.94s/it] 26%|â–ˆâ–ˆâ–Œ       | 13/50 [01:17<03:39,  5.94s/it] 28%|â–ˆâ–ˆâ–Š       | 14/50 [01:23<03:33,  5.94s/it] 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [01:29<03:27,  5.94s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [01:35<03:22,  5.94s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [01:41<03:16,  5.95s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [01:47<03:10,  5.95s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [01:53<03:04,  5.95s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [01:59<02:58,  5.95s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [02:05<02:52,  5.95s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [02:11<02:46,  5.95s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [02:17<02:40,  5.95s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [02:23<02:34,  5.95s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [02:29<02:28,  5.95s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [02:35<02:22,  5.95s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [02:41<02:16,  5.95s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [02:46<02:10,  5.95s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [02:52<02:04,  5.95s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [02:58<01:58,  5.95s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [03:04<01:52,  5.95s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [03:10<01:47,  5.95s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [03:16<01:41,  5.95s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [03:22<01:35,  5.95s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [03:28<01:29,  5.95s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [03:34<01:23,  5.95s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [03:40<01:17,  5.95s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [03:46<01:11,  5.95s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [03:52<01:05,  5.95s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [03:58<00:59,  5.95s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [04:04<00:53,  5.94s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [04:10<00:47,  5.94s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [04:16<00:41,  5.94s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [04:22<00:35,  5.94s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [04:28<00:29,  5.94s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [04:33<00:23,  5.94s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [04:39<00:17,  5.94s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [04:45<00:11,  5.94s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [04:51<00:05,  5.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [04:57<00:00,  5.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [04:57<00:00,  5.96s/it]
It is recommended to use `export_to_video` with `imageio` and `imageio-ffmpeg` as a backend. 
These libraries are not present in your environment. Attempting to use legacy OpenCV backend to export video. 
Support for the OpenCV backend will be deprecated in a future Diffusers version
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 202, in run
    result = wrapper.generate(image_path, text_prompt, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/cogvideox_inference.py", line 347, in generate
    result = self.service.generate_video(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/cogvideox_inference.py", line 216, in generate_video
    export_to_video(frames, str(output_path), fps=self.config.fps)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/cogvideox1.5-5b-i2v/lib/python3.10/site-packages/diffusers/utils/export_utils.py", line 154, in export_to_video
    return _legacy_export_to_video(video_frames, output_video_path, fps)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/cogvideox1.5-5b-i2v/lib/python3.10/site-packages/diffusers/utils/export_utils.py", line 121, in _legacy_export_to_video
    raise ImportError(BACKENDS_MAPPING["opencv"][1].format("export_to_video"))
ImportError: 
export_to_video requires the OpenCV library but it was not found in your environment. You can install it with pip: `pip
install opencv-python`

   âŒ cogvideox1.5-5b-i2v: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: sana-video-2b-480p
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating sana-video-2b-480p... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): sana-video-2b-480p

ğŸ” Verifying 1 model(s) for testing...
   âœ… sana-video-2b-480p: SANA-Video
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - sana-video-2b-480p
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: SANA-Video (sana-video-2b-480p)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with sana-video-2b-480p
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 177, in run
    wrapper_class = _load_model_wrapper(model_name)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 42, in _load_model_wrapper
    module = importlib.import_module(config["wrapper_module"])
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 879, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1017, in get_code
  File "<frozen importlib._bootstrap_external>", line 947, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/sana_inference.py", line 492
    }
    ^
SyntaxError: unmatched '}'
   âŒ sana-video-2b-480p: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: sana-video-2b-longlive
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating sana-video-2b-longlive... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): sana-video-2b-longlive

ğŸ” Verifying 1 model(s) for testing...
   âœ… sana-video-2b-longlive: SANA-Video
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - sana-video-2b-longlive
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: SANA-Video (sana-video-2b-longlive)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with sana-video-2b-longlive
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 177, in run
    wrapper_class = _load_model_wrapper(model_name)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 42, in _load_model_wrapper
    module = importlib.import_module(config["wrapper_module"])
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 879, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1017, in get_code
  File "<frozen importlib._bootstrap_external>", line 947, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/sana_inference.py", line 492
    }
    ^
SyntaxError: unmatched '}'
   âŒ sana-video-2b-longlive: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: wan-2.2-i2v-a14b
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating wan-2.2-i2v-a14b... (timeout: 10800s)

`torch_dtype` is deprecated! Use `dtype` instead!
ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): wan-2.2-i2v-a14b

ğŸ” Verifying 1 model(s) for testing...
   âœ… wan-2.2-i2v-a14b: WAN (Wan-AI)
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - wan-2.2-i2v-a14b
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: WAN (Wan-AI) (wan-2.2-i2v-a14b)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with wan-2.2-i2v-a14b
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: wan-2.2-i2v-a14b (will be reused for all tasks)
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 202, in run
    result = wrapper.generate(image_path, text_prompt, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/wan_inference.py", line 187, in generate
    result = self.wan_service.generate_video(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/wan_inference.py", line 107, in generate_video
    self._load_model()
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/wan_inference.py", line 49, in _load_model
    self.image_encoder = CLIPVisionModel.from_pretrained(
  File "/home/ubuntu/Hokin/VMEvalKit/envs/wan/lib/python3.10/site-packages/transformers/modeling_utils.py", line 277, in _wrapper
    return func(*args, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/wan/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4900, in from_pretrained
    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
  File "/home/ubuntu/Hokin/VMEvalKit/envs/wan/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1148, in _get_resolved_checkpoint_files
    raise OSError(
OSError: Wan-AI/Wan2.2-I2V-A14B-Diffusers does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.
   âŒ wan-2.2-i2v-a14b: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: luma-ray-2
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating luma-ray-2... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): luma-ray-2

ğŸ” Verifying 1 model(s) for testing...
   âœ… luma-ray-2: Luma Dream Machine
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - luma-ray-2
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: Luma Dream Machine (luma-ray-2)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with luma-ray-2
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 177, in run
    wrapper_class = _load_model_wrapper(model_name)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 42, in _load_model_wrapper
    module = importlib.import_module(config["wrapper_module"])
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/luma_inference.py", line 19, in <module>
    from ..utils.s3_uploader import S3ImageUploader
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/utils/__init__.py", line 3, in <module>
    from .video_decomposer import decompose_video
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/utils/video_decomposer.py", line 18, in <module>
    import cv2
ModuleNotFoundError: No module named 'cv2'
   âŒ luma-ray-2: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: luma-ray-flash-2
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating luma-ray-flash-2... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): luma-ray-flash-2

ğŸ” Verifying 1 model(s) for testing...
   âœ… luma-ray-flash-2: Luma Dream Machine
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - luma-ray-flash-2
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: Luma Dream Machine (luma-ray-flash-2)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with luma-ray-flash-2
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 177, in run
    wrapper_class = _load_model_wrapper(model_name)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 42, in _load_model_wrapper
    module = importlib.import_module(config["wrapper_module"])
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/luma_inference.py", line 19, in <module>
    from ..utils.s3_uploader import S3ImageUploader
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/utils/__init__.py", line 3, in <module>
    from .video_decomposer import decompose_video
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/utils/video_decomposer.py", line 18, in <module>
    import cv2
ModuleNotFoundError: No module named 'cv2'
   âŒ luma-ray-flash-2: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: veo-2
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating veo-2... (timeout: 10800s)

2025-12-17 04:50:26,207 | INFO     | VeoService initialized with model: veo-2.0-generate-001
2025-12-17 04:50:26,212 | INFO     | Padded image from 276Ã—276 to 490Ã—276 for 16:9 ratio
2025-12-17 04:50:26,216 | INFO     | Starting Veo video generation with model: veo-2.0-generate-001
2025-12-17 04:50:26,216 | INFO     | Prompt: Remove all cone objects from the scene. Do not do anything to other objects.
2025-12-17 04:50:26,686 | INFO     | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/veo-2.0-generate-001:predictLongRunning "HTTP/1.1 200 OK"
2025-12-17 04:50:26,686 | INFO     | Started operation: models/veo-2.0-generate-001/operations/wfvbl37qamdm
2025-12-17 04:50:26,687 | INFO     | Still running... attempt 1, elapsed: 0.0s
ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): veo-2

ğŸ” Verifying 1 model(s) for testing...
   âœ… veo-2: Google Veo
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - veo-2
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: Google Veo (veo-2)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with veo-2
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: veo-2 (will be reused for all tasks)
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 202, in run
    result = wrapper.generate(image_path, text_prompt, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/veo_inference.py", line 442, in generate
    video_bytes, metadata = asyncio.run(
  File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/usr/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/veo_inference.py", line 330, in generate_video
    operation = self.client.operations.get(name=operation_name)
TypeError: Operations.get() got an unexpected keyword argument 'name'
   âŒ veo-2: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: veo-3.0-generate
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating veo-3.0-generate... (timeout: 10800s)

2025-12-17 04:50:33,107 | INFO     | VeoService initialized with model: veo-3.0-generate-001
2025-12-17 04:50:33,113 | INFO     | Padded image from 276Ã—276 to 490Ã—276 for 16:9 ratio
2025-12-17 04:50:33,117 | INFO     | Starting Veo video generation with model: veo-3.0-generate-001
2025-12-17 04:50:33,117 | INFO     | Prompt: Remove all cone objects from the scene. Do not do anything to other objects.
2025-12-17 04:50:33,682 | INFO     | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/veo-3.0-generate-001:predictLongRunning "HTTP/1.1 200 OK"
2025-12-17 04:50:33,683 | INFO     | Started operation: models/veo-3.0-generate-001/operations/3uq6k8gplgbz
2025-12-17 04:50:33,683 | INFO     | Still running... attempt 1, elapsed: 0.0s
ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): veo-3.0-generate

ğŸ” Verifying 1 model(s) for testing...
   âœ… veo-3.0-generate: Google Veo
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - veo-3.0-generate
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: Google Veo (veo-3.0-generate)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with veo-3.0-generate
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: veo-3.0-generate (will be reused for all tasks)
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 202, in run
    result = wrapper.generate(image_path, text_prompt, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/veo_inference.py", line 442, in generate
    video_bytes, metadata = asyncio.run(
  File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/usr/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/veo_inference.py", line 330, in generate_video
    operation = self.client.operations.get(name=operation_name)
TypeError: Operations.get() got an unexpected keyword argument 'name'
   âŒ veo-3.0-generate: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: wavespeed-wan-2.1-i2v-480p
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âŒ Virtual environment not found for wavespeed-wan-2.1-i2v-480p

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: runway-gen4-turbo
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating runway-gen4-turbo... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): runway-gen4-turbo

ğŸ” Verifying 1 model(s) for testing...
   âœ… runway-gen4-turbo: Runway ML
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - runway-gen4-turbo
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: Runway ML (runway-gen4-turbo)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with runway-gen4-turbo
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: runway-gen4-turbo (will be reused for all tasks)
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 202, in run
    result = wrapper.generate(image_path, text_prompt, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/runway_inference.py", line 417, in generate
    result = asyncio.run(
  File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/usr/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/runway_inference.py", line 227, in generate_video
    image_url = await self._upload_image(processed_image_path)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/runway_inference.py", line 271, in _upload_image
    from ..utils.s3_uploader import S3ImageUploader
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/utils/__init__.py", line 2, in <module>
    from .s3_uploader import S3ImageUploader
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/utils/s3_uploader.py", line 6, in <module>
    import boto3
ModuleNotFoundError: No module named 'boto3'
   âŒ runway-gen4-turbo: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: openai-sora-2
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating openai-sora-2... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): openai-sora-2

ğŸ” Verifying 1 model(s) for testing...
   âœ… openai-sora-2: OpenAI Sora
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - openai-sora-2
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: OpenAI Sora (openai-sora-2)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with openai-sora-2
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 188, in run
    self._wrapper_cache[model_name] = wrapper_class(**init_kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/openai_inference.py", line 387, in __init__
    self.sora_service = SoraService(model=model)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/openai_inference.py", line 33, in __init__
    raise ValueError("OPENAI_API_KEY environment variable is required")
ValueError: OPENAI_API_KEY environment variable is required
   âŒ openai-sora-2: FAILED - see output above

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Validation Summary
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   âœ… Passed (3):
      âœ“ ltx-video
      âœ“ ltx-video-13b-distilled
      âœ“ svd

   âŒ Failed (18):
      âœ— morphic-frames-to-video
      âœ— hunyuan-video-i2v
      âœ— dynamicrafter-256
      âœ— dynamicrafter-512
      âœ— dynamicrafter-1024
      âœ— videocrafter2-512
      âœ— cogvideox-5b-i2v
      âœ— cogvideox1.5-5b-i2v
      âœ— sana-video-2b-480p
      âœ— sana-video-2b-longlive
      âœ— wan-2.2-i2v-a14b
      âœ— luma-ray-2
      âœ— luma-ray-flash-2
      âœ— veo-2
      âœ— veo-3.0-generate
      âœ— wavespeed-wan-2.1-i2v-480p
      âœ— runway-gen4-turbo
      âœ— openai-sora-2

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installation Summary
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   âœ… Installed (21):
      âœ“ ltx-video
      âœ“ ltx-video-13b-distilled
      âœ“ svd
      âœ“ morphic-frames-to-video
      âœ“ hunyuan-video-i2v
      âœ“ dynamicrafter-256
      âœ“ dynamicrafter-512
      âœ“ dynamicrafter-1024
      âœ“ videocrafter2-512
      âœ“ cogvideox-5b-i2v
      âœ“ cogvideox1.5-5b-i2v
      âœ“ sana-video-2b-480p
      âœ“ sana-video-2b-longlive
      âœ“ wan-2.2-i2v-a14b
      âœ“ luma-ray-2
      âœ“ luma-ray-flash-2
      âœ“ veo-2
      âœ“ veo-3.0-generate
      âœ“ wavespeed-wan-2.1-i2v-480p
      âœ“ runway-gen4-turbo
      âœ“ openai-sora-2

   âŒ Failed (1):
      âœ— veo-3.1-fast
